{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Swiss rail network exploration\\n\n",
    "\\n\n",
    "**NOTE: This notebook primarily analyzes the nationwide Swisstopo dataset for the Final Report.**\\n\n",
    "\\n\n",
    "It can load and analyse either:\\n\n",
    "\\n\n",
    "1. The **SBB infrastructure dataset** (Legacy/Incomplete) - Not used for final robustness analysis.\\n\n",
    "2. The **Nationwide `schienennetz_2056_de` geodatabase** from Swisstopo (Primary Source).\\n\n",
    "\\n\n",
    "Set `DATA_SOURCE` in the next cell to switch between them. For the final report, we use `swisstopo`.\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"swisstopo\"  # options: \"swisstopo\" or \"swisstopo\"\n",
    "EXPORT_GRAPH = True\n",
    "\n",
    "BASE_DIR = Path(\"../datasets/switzerland\")\n",
    "SBB_LINE_PATH = BASE_DIR / \"sbb-linie-mit-betriebspunkten.csv\"\n",
    "SBB_STATION_PATH = BASE_DIR / \"sbb-dienststellen-gemass-opentransportdataswiss.csv\"\n",
    "SWISSTOPO_GDB_PATH = BASE_DIR / \"schienennetz_2056_de.gdb\"\n",
    "\n",
    "GRAPH_OUTPUTS = {\n",
    "    \"sbb\": BASE_DIR / \"sbb_rail_network.gpickle\",\n",
    "    \"swisstopo\": BASE_DIR / \"swiss_rail_network_swisstopo.gpickle\",\n",
    "}\n",
    "\n",
    "LEGACY_GRAPH_PATHS = {\n",
    "    \"sbb\": BASE_DIR / \"swiss_rail_network.gpickle\",\n",
    "}\n",
    "\n",
    "station_metadata = pd.read_csv(SBB_STATION_PATH, sep=';')\n",
    "station_metadata['abbreviation_clean'] = (\n",
    "    station_metadata['abbreviation']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    ")\n",
    "stop_point_mask = station_metadata['stopPoint'].astype(str).str.lower() == 'true'\n",
    "station_abbreviation_set = set(\n",
    "    station_metadata.loc[stop_point_mask, 'abbreviation_clean'].dropna().tolist()\n",
    ")\n",
    "station_abbreviation_set.discard('NAN')\n",
    "\n",
    "GRAPH_OUTPUT_PATH = GRAPH_OUTPUTS[DATA_SOURCE]\n",
    "LEGACY_PATH = LEGACY_GRAPH_PATHS.get(DATA_SOURCE)\n",
    "if LEGACY_PATH and LEGACY_PATH.exists() and not GRAPH_OUTPUT_PATH.exists():\n",
    "    print(f\"Legacy graph detected at {LEGACY_PATH}. New exports will use {GRAPH_OUTPUT_PATH}.\")\n",
    "print(f\"Using data source: {DATA_SOURCE}\")\n",
    "print(f\"Graph exports will be written to: {GRAPH_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SOURCE == \"sbb\":\n",
    "    sbb_line_data = pd.read_csv(SBB_LINE_PATH, sep=';')\n",
    "    sbb_stations_data = pd.read_csv(SBB_STATION_PATH, sep=';')\n",
    "    sbb_line_and_station_data = sbb_line_data.merge(\n",
    "        sbb_stations_data,\n",
    "        left_on=\"Didok number\",\n",
    "        right_on=\"number\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_didok\"),\n",
    "    )\n",
    "\n",
    "    print(f\"Line table shape: {sbb_line_data.shape}\")\n",
    "    print(f\"Stations table shape: {sbb_stations_data.shape}\")\n",
    "    print(\"Combined columns:\")\n",
    "    print(sorted(sbb_line_and_station_data.columns)[:10], \"...\")\n",
    "else:\n",
    "    net_segments = gpd.read_file(SWISSTOPO_GDB_PATH, layer='Netzsegment')\n",
    "    net_nodes = gpd.read_file(SWISSTOPO_GDB_PATH, layer='Netzknoten')\n",
    "\n",
    "    print(f\"Segments table shape: {net_segments.shape}\")\n",
    "    print(f\"Nodes table shape: {net_nodes.shape}\")\n",
    "    print(\"Segment columns:\")\n",
    "    print(sorted(net_segments.columns))\n",
    "    print(\"Node columns:\")\n",
    "    print(sorted(net_nodes.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_geopos(value):\n",
    "    if isinstance(value, str) and ',' in value:\n",
    "        lat_str, lon_str = value.split(',', 1)\n",
    "        try:\n",
    "            lat = float(lat_str.strip())\n",
    "            lon = float(lon_str.strip())\n",
    "        except ValueError:\n",
    "            return None\n",
    "        if -90 <= lat <= 90 and -180 <= lon <= 180:\n",
    "            return lat, lon\n",
    "    return None\n",
    "\n",
    "\n",
    "def classify_station(abbreviation, fallback_rows=None):\n",
    "    abbr = (abbreviation or \"\").strip().upper()\n",
    "    if abbr and abbr in station_abbreviation_set:\n",
    "        return True\n",
    "    fallback_rows = fallback_rows or []\n",
    "    for row in fallback_rows:\n",
    "        row_abbr = row.get('Station abbreviation')\n",
    "        if isinstance(row_abbr, str) and row_abbr.strip().upper() in station_abbreviation_set:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def flatten_lines(geom):\n",
    "    if geom is None or geom.is_empty:\n",
    "        return []\n",
    "    coords = []\n",
    "    if geom.geom_type == 'LineString':\n",
    "        coords.extend((pt[1], pt[0]) for pt in geom.coords)\n",
    "    elif geom.geom_type == 'MultiLineString':\n",
    "        for line in geom.geoms:\n",
    "            coords.extend((pt[1], pt[0]) for pt in line.coords)\n",
    "    return coords\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "node_records = []\n",
    "positions = {}\n",
    "\n",
    "if DATA_SOURCE == \"sbb\":\n",
    "    LINE_COL = \"Line\"\n",
    "    STATION_COL = \"Station abbreviation\"\n",
    "    ORDER_COL = \"KM\"\n",
    "\n",
    "    node_groups = sbb_line_and_station_data.dropna(subset=[STATION_COL]).groupby(STATION_COL)\n",
    "    for station, group in node_groups:\n",
    "        row_dicts = group.to_dict('records')\n",
    "        coords = None\n",
    "        for row in row_dicts:\n",
    "            coords = parse_geopos(row.get('Geopos')) or parse_geopos(row.get('Geopos_didok'))\n",
    "            if coords:\n",
    "                break\n",
    "        lat, lon = coords if coords else (None, None)\n",
    "        label = next((row.get('Stop name') for row in row_dicts if isinstance(row.get('Stop name'), str) and row.get('Stop name')), station)\n",
    "        is_station = classify_station(station, row_dicts)\n",
    "        node_attrs = {\n",
    "            'label': label,\n",
    "            'abbreviation': station,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'is_station': is_station,\n",
    "            'rows': row_dicts,\n",
    "            'source': 'sbb',\n",
    "        }\n",
    "        G.add_node(station, **node_attrs)\n",
    "        positions[station] = (lat, lon)\n",
    "        node_records.append({\n",
    "            'node_id': station,\n",
    "            'label': label,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'is_station': is_station,\n",
    "            'source': 'sbb',\n",
    "        })\n",
    "\n",
    "    ordered_df = (\n",
    "        sbb_line_and_station_data\n",
    "        .dropna(subset=[LINE_COL, STATION_COL, ORDER_COL])\n",
    "        .sort_values([LINE_COL, ORDER_COL])\n",
    "    )\n",
    "\n",
    "    for line_id, group in ordered_df.groupby(LINE_COL):\n",
    "        stops = group[STATION_COL].tolist()\n",
    "        kms = group[ORDER_COL].tolist()\n",
    "        row_dicts = group.to_dict('records')\n",
    "\n",
    "        for idx, (u, v) in enumerate(zip(stops[:-1], stops[1:])):\n",
    "            segment_meta = {\n",
    "                'line_id': line_id,\n",
    "                'order_index': idx,\n",
    "                'from_station': u,\n",
    "                'to_station': v,\n",
    "                'from_km': kms[idx],\n",
    "                'to_km': kms[idx + 1],\n",
    "                'from_row': row_dicts[idx],\n",
    "                'to_row': row_dicts[idx + 1],\n",
    "            }\n",
    "            if G.has_edge(u, v):\n",
    "                G[u][v]['lines'].add(line_id)\n",
    "                G[u][v]['segments'].append(segment_meta)\n",
    "            else:\n",
    "                G.add_edge(\n",
    "                    u,\n",
    "                    v,\n",
    "                    lines={line_id},\n",
    "                    segments=[segment_meta],\n",
    "                    source='sbb',\n",
    "                )\n",
    "\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        data['lines'] = sorted(data['lines'])\n",
    "else:\n",
    "    nodes_gdf = net_nodes.to_crs(4326)\n",
    "    segments_gdf = net_segments\n",
    "    segments_wgs84 = segments_gdf.to_crs(4326)\n",
    "\n",
    "    for _, row in nodes_gdf.iterrows():\n",
    "        node_id = row['xtf_id']\n",
    "        abbr = row.get('Betriebspunkt_Abkuerzung')\n",
    "        label = abbr or row.get('Betriebspunkt_Name') or node_id\n",
    "        lat = row.geometry.y\n",
    "        lon = row.geometry.x\n",
    "        is_station = False\n",
    "        if isinstance(abbr, str) and abbr.strip():\n",
    "            is_station = abbr.strip().upper() in station_abbreviation_set\n",
    "        node_attrs = {\n",
    "            'label': label,\n",
    "            'abbreviation': abbr,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'is_station': is_station,\n",
    "            'rows': [row.drop(labels='geometry').to_dict()],\n",
    "            'source': 'swisstopo',\n",
    "        }\n",
    "        G.add_node(node_id, **node_attrs)\n",
    "        positions[node_id] = (lat, lon)\n",
    "        node_records.append({\n",
    "            'node_id': node_id,\n",
    "            'label': label,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'is_station': is_station,\n",
    "            'source': 'swisstopo',\n",
    "        })\n",
    "\n",
    "    for _, row in segments_wgs84.iterrows():\n",
    "        u = row['rAnfangsknoten']\n",
    "        v = row['rEndknoten']\n",
    "        if pd.isna(u) or pd.isna(v):\n",
    "            continue\n",
    "        if u not in G.nodes or v not in G.nodes:\n",
    "            continue\n",
    "        lines = [row['Name']] if isinstance(row.get('Name'), str) else []\n",
    "        segment_meta = {\n",
    "            'segment_id': row['xtf_id'],\n",
    "            'line_name': row.get('Name'),\n",
    "            'track_count': row.get('AnzahlStreckengleise'),\n",
    "            'gauge': row.get('Spurweite'),\n",
    "            'electrified': row.get('Elektrifizierung'),\n",
    "            'coords_wgs84': flatten_lines(row.geometry),\n",
    "        }\n",
    "        if G.has_edge(u, v):\n",
    "            combined = set(G[u][v]['lines'])\n",
    "            combined.update(lines)\n",
    "            G[u][v]['lines'] = sorted(combined)\n",
    "            G[u][v]['segments'].append(segment_meta)\n",
    "        else:\n",
    "            G.add_edge(\n",
    "                u,\n",
    "                v,\n",
    "                lines=sorted(lines),\n",
    "                segments=[segment_meta],\n",
    "                source='swisstopo',\n",
    "            )\n",
    "\n",
    "node_summary_df = pd.DataFrame(node_records)\n",
    "print(f\"Nodes prepared with coordinates: {len(node_summary_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"Edges: {G.number_of_edges():,}\")\n",
    "print(f\"Connected components: {nx.number_connected_components(G):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(nx.connected_components(G))\n",
    "rows = []\n",
    "for nodeset in components:\n",
    "    sub = G.subgraph(nodeset)\n",
    "    rows.append({\n",
    "        'num_nodes': sub.number_of_nodes(),\n",
    "        'num_edges': sub.number_of_edges(),\n",
    "    })\n",
    "cc_df = pd.DataFrame(rows)\n",
    "cc_df = cc_df.sort_values('num_nodes', ascending=False).reset_index(drop=True)\n",
    "cc_df.insert(0, 'rank', cc_df.index + 1)\n",
    "cc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_by_size = sorted(components, key=len, reverse=True)\n",
    "if len(components_by_size) > 1:\n",
    "    small_components = components_by_size[1:]\n",
    "else:\n",
    "    small_components = []\n",
    "\n",
    "rows = []\n",
    "component_meta = {}\n",
    "for rank, nodeset in enumerate(small_components, start=2):\n",
    "    sub = G.subgraph(nodeset)\n",
    "    component_meta[rank] = {\n",
    "        'num_nodes': sub.number_of_nodes(),\n",
    "        'num_edges': sub.number_of_edges(),\n",
    "    }\n",
    "    for node_id in sorted(nodeset):\n",
    "        data = G.nodes[node_id]\n",
    "        rows.append({\n",
    "            'component_rank': rank,\n",
    "            'node_id': node_id,\n",
    "            'label': data.get('label', node_id),\n",
    "            'abbreviation': data.get('abbreviation'),\n",
    "            'is_station': data.get('is_station', False),\n",
    "            'source': data.get('source', DATA_SOURCE),\n",
    "        })\n",
    "\n",
    "small_cc_df = (\n",
    "    pd.DataFrame(rows)\n",
    "    .sort_values(['component_rank', 'label'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "small_cc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_rows = []\n",
    "singleton_rows = []\n",
    "\n",
    "if small_cc_df.empty:\n",
    "    print(\"No satellite components detected outside the largest component.\")\n",
    "else:\n",
    "    for comp_rank, group in small_cc_df.groupby('component_rank'):\n",
    "        group = group.sort_values('label')\n",
    "        meta = component_meta.get(comp_rank, {'num_nodes': len(group), 'num_edges': max(len(group) - 1, 0)})\n",
    "        if len(group) < 2:\n",
    "            singleton_rows.append({\n",
    "                'component_rank': comp_rank,\n",
    "                'num_nodes': meta['num_nodes'],\n",
    "                'num_edges': meta['num_edges'],\n",
    "                'node_id': group.iloc[0]['node_id'],\n",
    "                'label': group.iloc[0]['label'],\n",
    "            })\n",
    "            continue\n",
    "        a, b = group.iloc[0], group.iloc[1]\n",
    "        pair_rows.append({\n",
    "            'component_rank': comp_rank,\n",
    "            'num_nodes': meta['num_nodes'],\n",
    "            'num_edges': meta['num_edges'],\n",
    "            'node1': f\"{a['node_id']} ({a['label']})\",\n",
    "            'node2': f\"{b['node_id']} ({b['label']})\",\n",
    "        })\n",
    "\n",
    "small_pairs_df = (\n",
    "    pd.DataFrame(pair_rows)\n",
    "    .sort_values('component_rank')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "singleton_components_df = (\n",
    "    pd.DataFrame(singleton_rows)\n",
    "    .sort_values('component_rank')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Pair summary:\")\n",
    "print(small_pairs_df)\n",
    "print(\"Singleton components:\")\n",
    "print(singleton_components_df if not singleton_components_df.empty else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if components_by_size:\n",
    "    largest_nodes = components_by_size[0]\n",
    "    print(f\"Largest component nodes: {len(largest_nodes):,}\")\n",
    "    print(f\"Largest component share: {len(largest_nodes) / G.number_of_nodes():.2%}\")\n",
    "else:\n",
    "    print(\"Graph has no components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_GRAPH:\n",
    "    GRAPH_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with GRAPH_OUTPUT_PATH.open('wb') as f:\n",
    "        pickle.dump(G, f)\n",
    "    print(f\"Serialized graph to {GRAPH_OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"Skipping graph export.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
