{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "station_path = '../datasets/japan/N02-24_GML/UTF-8/N02-24_Station.geojson'\n",
    "railroad_path = '../datasets/japan/N02-24_GML/UTF-8/N02-24_RailroadSection.geojson'\n",
    "\n",
    "# Load data\n",
    "print(\"Loading stations...\")\n",
    "gdf_stations = gpd.read_file(station_path)\n",
    "print(f\"Loaded {len(gdf_stations)} stations.\")\n",
    "\n",
    "print(\"Loading railroad sections...\")\n",
    "gdf_railroads = gpd.read_file(railroad_path)\n",
    "print(f\"Loaded {len(gdf_railroads)} railroad sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Understanding the Data Structure\n",
    "\n",
    "**Difference between the files:**\n",
    "*   **`N02-24_Station.geojson`**: Contains **LineString** geometries representing the platforms of stations. Attributes include Station Name (`N02_005`) and Operator (`N02_004`).\n",
    "*   **`N02-24_RailroadSection.geojson`**: Contains **LineString** geometries representing the physical railway tracks. Attributes include Line Name (`N02_003`) and Operator (`N02_004`).\n",
    "\n",
    "**The Connectivity Challenge:**\n",
    "For a NetworkX graph, we need explicit \"Node A -> Node B\" connections. This dataset provides the *visual* lines. To build a graph, we must verify if the endpoints of the `RailroadSection` lines spatially coincide with the `Station` platform lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows to check attributes\n",
    "\n",
    "# (N02_001) Railway classification\tDifferentiation by type of railway line.\n",
    "# (N02_002) Business type\t        Differentiation by railway line operators.\n",
    "# (N02_003) Route name\t            Name of the railway line\n",
    "# (N02_004) Operating company\t    A company that operates railway lines.\n",
    "# (N02_005) Station name\t        Name of the station\n",
    "# (N02_005c) Station code\t        The unique number added by sorting the latitude of the station in descending order\n",
    "# (N02_005g) Group code\t            Group code A station within 300m and a station with the same name as a group, and the station code closest to the center of gravity of the group\n",
    "display(gdf_stations.head(3))\n",
    "\n",
    "# (N02_001) Railway classification\tDifferentiation by type of railway line.\n",
    "# (N02_002) Business type\t        Differentiation by railway line operators.\n",
    "# (N02_003) Route name\t            Name of the railway line\n",
    "# (N02_004) Operating company       A company that operates railway lines.\n",
    "display(gdf_railroads.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Visualizing the Network\n",
    "We will plot a subset of the data (e.g., Tokyo area) to visually inspect the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for a specific area (e.g., Tokyo) to keep the map responsive\n",
    "# Tokyo coordinates approx: 35.68, 139.76\n",
    "# We'll use a bounding box or just plot the first N features for a quick check if the dataset is huge.\n",
    "# However, let's try to plot the whole thing statically first.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "gdf_railroads.plot(ax=ax, color='gray', linewidth=0.5, alpha=0.7, label='Railroads')\n",
    "gdf_stations.plot(ax=ax, color='red', markersize=2, label='Stations')\n",
    "plt.title(\"Japanese Railway Network (Static View)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Interactive Map (Folium)\n",
    "Let's zoom in to see if the lines actually touch the stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered on Tokyo\n",
    "m = folium.Map(location=[35.6812, 139.7671], zoom_start=12, tiles='CartoDB Positron')\n",
    "\n",
    "sample_rail = gdf_railroads\n",
    "sample_stations = gdf_stations\n",
    "\n",
    "folium.GeoJson(\n",
    "    sample_rail,\n",
    "    name='Railroads',\n",
    "    style_function=lambda x: {'color': 'blue', 'weight': 2, 'opacity': 0.6}\n",
    ").add_to(m)\n",
    "\n",
    "folium.GeoJson(\n",
    "    sample_stations,\n",
    "    name='Stations',\n",
    "    style_function=lambda x: {'color': 'red', 'fillColor': 'red', 'radius': 4}\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Topological Assessment\n",
    "We check if the station coordinates are *exactly* on the line endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all start and end points of the railroad sections\n",
    "rail_endpoints = []\n",
    "for line in gdf_railroads.geometry:\n",
    "    if line.geom_type == 'LineString':\n",
    "        coords = list(line.coords)\n",
    "        rail_endpoints.append(coords[0]) # Start\n",
    "        rail_endpoints.append(coords[-1]) # End\n",
    "\n",
    "rail_endpoints_set = set(rail_endpoints)\n",
    "\n",
    "# Check how many stations match these endpoints exactly\n",
    "exact_matches = 0\n",
    "total_stations = len(gdf_stations)\n",
    "\n",
    "for points in gdf_stations.geometry:\n",
    "    # Check if (x, y) is in the endpoints set\n",
    "    for (x, y) in points.coords:\n",
    "        point = Point(x, y)\n",
    "        if (point.x, point.y) in rail_endpoints_set:\n",
    "            exact_matches += 1\n",
    "\n",
    "print(f\"Total Stations: {total_stations}\")\n",
    "print(f\"Stations exactly matching rail endpoints: {exact_matches}\")\n",
    "print(f\"Percentage: {exact_matches / total_stations * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Building the NetworkX Graph\n",
    "\n",
    "We'll now construct a NetworkX graph by:\n",
    "1. Grouping station platforms by name to create station nodes\n",
    "2. Using exact coordinate matching to map railroad endpoints to stations\n",
    "3. Snapping railroad geometries to station centroids\n",
    "4. Exporting to `.gpickle` format for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Use pre-computed Group Code (N02_005g) for clustering\n",
    "# The group code groups stations within 300m that share the same name\n",
    "\n",
    "print(\"Step 1: Group station platforms by Group Code (N02_005g)\")\n",
    "\n",
    "group_code_groups = defaultdict(list)\n",
    "for idx, station in gdf_stations.iterrows():\n",
    "    group_code = station['N02_005g']\n",
    "    group_code_groups[group_code].append({\n",
    "        'idx': idx,\n",
    "        'geometry': station.geometry,\n",
    "        'name': station['N02_005'],\n",
    "        'operator': station.get('N02_004'),\n",
    "        'station_code': station.get('N02_005c'),\n",
    "    })\n",
    "\n",
    "print(f\"Found {len(group_code_groups)} unique group codes\")\n",
    "\n",
    "# Step 2: Check for large centroid deviations (sanity check)\n",
    "print(\"\\nStep 2: Sanity check - verify group codes are spatially coherent:\")\n",
    "SPREAD_THRESHOLD_KM = 1\n",
    "\n",
    "problematic_groups = []\n",
    "for group_code, platforms in group_code_groups.items():\n",
    "    if len(platforms) < 2:\n",
    "        continue\n",
    "    \n",
    "    all_coords = []\n",
    "    for p in platforms:\n",
    "        all_coords.extend(list(p['geometry'].coords))\n",
    "    \n",
    "    lons = [c[0] for c in all_coords]\n",
    "    lats = [c[1] for c in all_coords]\n",
    "    \n",
    "    lat_spread_km = (max(lats) - min(lats)) * 111\n",
    "    lon_spread_km = (max(lons) - min(lons)) * 111 * 0.8\n",
    "    max_spread_km = max(lat_spread_km, lon_spread_km)\n",
    "    \n",
    "    if max_spread_km > SPREAD_THRESHOLD_KM:\n",
    "        names = set(p['name'] for p in platforms)\n",
    "        problematic_groups.append((group_code, max_spread_km, len(platforms), names))\n",
    "\n",
    "if problematic_groups:\n",
    "    print(f\"WARNING: {len(problematic_groups)} group codes have platforms spread > {SPREAD_THRESHOLD_KM}km apart:\")\n",
    "    for group_code, spread, count, names in sorted(problematic_groups, key=lambda x: -x[1])[:10]:\n",
    "        print(f\"    Group '{group_code}': {spread:.2f}km spread, {count} platforms, names: {names}\")\n",
    "else:\n",
    "    print(\"✓ All group codes are spatially coherent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import unary_union\n",
    "from collections import Counter\n",
    "\n",
    "# Step 3: Create station nodes from group codes\n",
    "print(\"Step 3: Create station nodes\")\n",
    "station_groups = {}\n",
    "coord_to_station = {}\n",
    "\n",
    "for group_code, platforms in group_code_groups.items():\n",
    "    station_id = group_code  # Use group code as station ID\n",
    "    \n",
    "    all_platform_geoms = [p['geometry'] for p in platforms]\n",
    "    combined_platforms = unary_union(all_platform_geoms)\n",
    "    centroid = combined_platforms.centroid\n",
    "    \n",
    "    # Get all unique names and operators in this group\n",
    "    names = list(set(p['name'] for p in platforms if p['name']))\n",
    "    operators = list(set(p['operator'] for p in platforms if p['operator']))\n",
    "    coords = list(set([coord for p in platforms for coord in p['geometry'].coords]))\n",
    "    \n",
    "    # Use the most common name as the display name\n",
    "    name_counts = Counter(p['name'] for p in platforms if p['name'])\n",
    "    display_name = max(name_counts, key=name_counts.get) if name_counts else f\"Station_{group_code}\"\n",
    "    \n",
    "    station_groups[station_id] = {\n",
    "        'centroid': centroid,\n",
    "        'coords': coords,\n",
    "        'geometry': combined_platforms,\n",
    "        'lat': centroid.y,\n",
    "        'lon': centroid.x,\n",
    "        'platforms': combined_platforms,\n",
    "        'name': display_name,\n",
    "        'all_names': names,\n",
    "        'operators': operators,\n",
    "        'platform_count': len(platforms),\n",
    "        'group_code': group_code,\n",
    "    }\n",
    "\n",
    "for station_id, station in station_groups.items():\n",
    "    for coord in station['coords']:\n",
    "        if coord in coord_to_station:\n",
    "            print(f\"Warning: Coordinate {coord} already mapped to station {coord_to_station[coord]}\")\n",
    "        coord_to_station[coord] = station_id\n",
    "\n",
    "print(f\"Created {len(station_groups)} station nodes\")\n",
    "print(f\"Total platform coordinates mapped: {len(coord_to_station)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Detect and merge interchange stations (shared coordinates)\n",
    "print(\"Step 4: Detect and merge interchange stations\")\n",
    "\n",
    "# Find coordinates shared by multiple station groups\n",
    "coord_to_groups = defaultdict(set)\n",
    "for group_code, data in station_groups.items():\n",
    "    for coord in data['coords']:\n",
    "        coord_to_groups[coord].add(group_code)\n",
    "\n",
    "# Find coordinates shared by multiple groups\n",
    "shared_coords = {coord: groups for coord, groups in coord_to_groups.items() if len(groups) > 1}\n",
    "\n",
    "if shared_coords:\n",
    "    # Build interchange groups using Union-Find\n",
    "    interchange_pairs = defaultdict(set)\n",
    "    for coord, groups in shared_coords.items():\n",
    "        groups_list = list(groups)\n",
    "        for i in range(len(groups_list)):\n",
    "            for j in range(i + 1, len(groups_list)):\n",
    "                interchange_pairs[groups_list[i]].add(groups_list[j])\n",
    "                interchange_pairs[groups_list[j]].add(groups_list[i])\n",
    "    \n",
    "    print(f\"Found {len(interchange_pairs)} station groups with shared coordinates\")\n",
    "    \n",
    "    # Union-Find to merge connected interchange stations\n",
    "    parent = {code: code for code in station_groups}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    # Union all interchange pairs\n",
    "    for group_code, partners in interchange_pairs.items():\n",
    "        for partner in partners:\n",
    "            union(group_code, partner)\n",
    "    \n",
    "    # Group by root\n",
    "    root_to_members = defaultdict(set)\n",
    "    for code in station_groups:\n",
    "        root_to_members[find(code)].add(code)\n",
    "    \n",
    "    # Merge station groups that share coordinates\n",
    "    merged_count = 0\n",
    "    for root, member_codes in root_to_members.items():\n",
    "        if len(member_codes) > 1:\n",
    "            merged_count += 1\n",
    "            # Pick the shortest name as the representative\n",
    "            representative = min(member_codes, key=lambda c: len(station_groups[c]['name']))\n",
    "            \n",
    "            # Collect all data from member groups\n",
    "            all_coords = list(set(coord for code in member_codes for coord in station_groups[code]['coords']))\n",
    "            all_names = list(set(name for code in member_codes for name in station_groups[code]['all_names']))\n",
    "            all_operators = list(set(op for code in member_codes for op in station_groups[code]['operators']))\n",
    "            all_platform_geoms = [station_groups[code]['platforms'] for code in member_codes]\n",
    "            combined_platforms = unary_union(all_platform_geoms)\n",
    "            centroid = combined_platforms.centroid\n",
    "            total_platforms = sum(station_groups[code]['platform_count'] for code in member_codes)\n",
    "            \n",
    "            # Update representative with merged data\n",
    "            station_groups[representative] = {\n",
    "                'centroid': centroid,\n",
    "                'lat': centroid.y,\n",
    "                'lon': centroid.x,\n",
    "                'geometry': combined_platforms,\n",
    "                'platforms': combined_platforms,\n",
    "                'name': station_groups[representative]['name'],\n",
    "                'all_names': all_names,\n",
    "                'operators': all_operators,\n",
    "                'platform_count': total_platforms,\n",
    "                'coords': all_coords,\n",
    "                'group_code': representative,\n",
    "                'merged_from': list(member_codes),\n",
    "            }\n",
    "            \n",
    "            # Update coord_to_station mapping\n",
    "            for coord in all_coords:\n",
    "                coord_to_station[coord] = representative\n",
    "            \n",
    "            # Remove non-representative groups\n",
    "            for code in member_codes:\n",
    "                if code != representative:\n",
    "                    del station_groups[code]\n",
    "            \n",
    "            print(f\"  Merged {list(member_codes)} -> '{station_groups[representative]['name']}' ({representative})\")\n",
    "    \n",
    "    print(f\"\\nMerged {merged_count} interchange groups\")\n",
    "    print(f\"Station groups after merging: {len(station_groups)}\")\n",
    "else:\n",
    "    print(\"✓ No stations share coordinates - no merging needed\")\n",
    "\n",
    "print(f\"\\nFinal station count: {len(station_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build the NetworkX graph with spatial snapping for small imprecision in coordinates\n",
    "print(\"Step 5: Build NetworkX graph\")\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add all station nodes\n",
    "for station_id, data in station_groups.items():\n",
    "    G.add_node(\n",
    "        station_id,\n",
    "        node_type='station',\n",
    "        lat=data['lat'],\n",
    "        lon=data['lon'],\n",
    "        name=data['name'],\n",
    "        all_names=data.get('all_names', [data['name']]),\n",
    "        operators=data.get('operators', []),\n",
    "        platform_count=data.get('platform_count', 1),\n",
    "        group_code=data.get('group_code', station_id),\n",
    "        merged_from=data.get('merged_from', []),\n",
    "    )\n",
    "\n",
    "print(f\"Added {len(station_groups)} station nodes\")\n",
    "\n",
    "# Build KD-tree of all station coordinates for spatial snapping\n",
    "all_station_coords = []\n",
    "coord_to_station_list = []  # parallel list for lookups\n",
    "for station_id, data in station_groups.items():\n",
    "    for coord in data['coords']:\n",
    "        all_station_coords.append(coord)\n",
    "        coord_to_station_list.append(station_id)\n",
    "\n",
    "station_coord_array = np.array(all_station_coords)\n",
    "station_tree = cKDTree(station_coord_array)\n",
    "\n",
    "# Snapping threshold: ~20cm (in degrees, roughly 2e-6 at Japan's latitude)\n",
    "# 1 degree ≈ 111km, so 0.2m ≈ 0.2/111000 ≈ 1.8e-6 degrees\n",
    "SNAP_THRESHOLD_DEG = 2e-6  # ~20cm\n",
    "print(f\"Spatial snapping threshold: {SNAP_THRESHOLD_DEG}° (~{SNAP_THRESHOLD_DEG * 111000 * 100:.1f}cm)\")\n",
    "\n",
    "infra_node_counter = 0\n",
    "infra_nodes = {}\n",
    "infra_coords_list = []  # For KD-tree of infrastructure nodes\n",
    "infra_tree = None\n",
    "\n",
    "matched_endpoints = 0\n",
    "snapped_endpoints = 0\n",
    "unmatched_endpoints = 0\n",
    "skipped_self_loops = 0\n",
    "\n",
    "def get_or_create_infra_node(coord):\n",
    "    \"\"\"Get existing infra node or create new one, with spatial snapping.\"\"\"\n",
    "    global infra_node_counter, infra_tree\n",
    "    \n",
    "    # Check if there's already an infra node nearby\n",
    "    if infra_coords_list:\n",
    "        if infra_tree is None or len(infra_coords_list) > len(infra_tree.data):\n",
    "            infra_tree = cKDTree(np.array(infra_coords_list))\n",
    "        dist, idx = infra_tree.query(coord)\n",
    "        if dist <= SNAP_THRESHOLD_DEG:\n",
    "            return list(infra_nodes.values())[idx]\n",
    "    \n",
    "    # Create new infra node\n",
    "    infra_node_id = f\"INFRA_{infra_node_counter}\"\n",
    "    infra_node_counter += 1\n",
    "    infra_nodes[coord] = infra_node_id\n",
    "    infra_coords_list.append(coord)\n",
    "    G.add_node(infra_node_id, node_type='infrastructure', lat=coord[1], lon=coord[0])\n",
    "    return infra_node_id\n",
    "\n",
    "for idx, rail in gdf_railroads.iterrows():\n",
    "    if rail.geometry.geom_type != 'LineString':\n",
    "        continue\n",
    "    \n",
    "    coords = list(rail.geometry.coords)\n",
    "    start_coord = coords[0]\n",
    "    end_coord = coords[-1]\n",
    "    \n",
    "    # Try exact match first, then spatial snapping to stations\n",
    "    def resolve_endpoint(coord):\n",
    "        global matched_endpoints, snapped_endpoints, unmatched_endpoints\n",
    "        \n",
    "        # Exact match\n",
    "        if coord in coord_to_station:\n",
    "            matched_endpoints += 1\n",
    "            return coord_to_station[coord]\n",
    "        \n",
    "        # Spatial snapping to stations\n",
    "        dist, idx = station_tree.query(coord)\n",
    "        if dist <= SNAP_THRESHOLD_DEG:\n",
    "            snapped_endpoints += 1\n",
    "            return coord_to_station_list[idx]\n",
    "        \n",
    "        # Create or find infrastructure node (with snapping)\n",
    "        unmatched_endpoints += 1\n",
    "        return get_or_create_infra_node(coord)\n",
    "    \n",
    "    start_node = resolve_endpoint(start_coord)\n",
    "    end_node = resolve_endpoint(end_coord)\n",
    "    \n",
    "    if start_node == end_node:\n",
    "        skipped_self_loops += 1\n",
    "        continue\n",
    "    \n",
    "    line_name = rail.get('N02_003') if 'N02_003' in rail.index else None\n",
    "    operator = rail.get('N02_004') if 'N02_004' in rail.index else None\n",
    "    \n",
    "    if G.has_edge(start_node, end_node):\n",
    "        if line_name and line_name not in G[start_node][end_node]['lines']:\n",
    "            G[start_node][end_node]['lines'].append(line_name)\n",
    "    else:\n",
    "        G.add_edge(start_node, end_node, lines=[line_name] if line_name else [], operator=operator)\n",
    "\n",
    "print(f\"\\nGraph construction complete:\")\n",
    "print(f\"  Total nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"  - Station nodes: {len(station_groups):,}\")\n",
    "print(f\"  - Infrastructure nodes: {len(infra_nodes):,}\")\n",
    "print(f\"  Total edges: {G.number_of_edges():,}\")\n",
    "print(f\"  Skipped self-loops: {skipped_self_loops:,}\")\n",
    "print(f\"\\nEndpoint matching:\")\n",
    "print(f\"  Exact matches: {matched_endpoints:,}\")\n",
    "print(f\"  Snapped (within {SNAP_THRESHOLD_DEG * 111000 * 100:.0f}cm): {snapped_endpoints:,}\")\n",
    "print(f\"  Unmatched (infrastructure): {unmatched_endpoints:,}\")\n",
    "total_endpoints = matched_endpoints + snapped_endpoints + unmatched_endpoints\n",
    "print(f\"  Station match rate: {(matched_endpoints + snapped_endpoints) / total_endpoints * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 6: Analyze graph structure\")\n",
    "\n",
    "num_components = nx.number_connected_components(G)\n",
    "components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "\n",
    "print(f\"Connected components: {num_components}\")\n",
    "if num_components > 0:\n",
    "    largest = components[0]\n",
    "    print(f\"Largest component: {len(largest):,} nodes ({len(largest)/G.number_of_nodes()*100:.1f}%)\")\n",
    "    \n",
    "    # Count single-node station components\n",
    "    single_station_components = [c for c in components if len(c) == 1 and G.nodes[list(c)[0]].get('node_type') == 'station']\n",
    "    if single_station_components:\n",
    "        print(f\"\\nWARNING: {len(single_station_components)} stations are isolated (single-node components):\")\n",
    "        for c in single_station_components[:10]:\n",
    "            node = list(c)[0]\n",
    "            node_data = G.nodes[node]\n",
    "            print(f\"    '{node}' ({node_data.get('name', 'unknown')})\")\n",
    "        if len(single_station_components) > 10:\n",
    "            print(f\"    ... and {len(single_station_components) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 6b: Diagnosing isolated stations\")\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "\n",
    "# Get all isolated stations\n",
    "isolated_stations = [list(c)[0] for c in components if len(c) == 1 and G.nodes[list(c)[0]].get('node_type') == 'station']\n",
    "\n",
    "# Build a KD-tree of all railroad endpoints\n",
    "all_rail_endpoints = []\n",
    "endpoint_to_rail_idx = {}\n",
    "for idx, rail in gdf_railroads.iterrows():\n",
    "    if rail.geometry.geom_type != 'LineString':\n",
    "        continue\n",
    "    coords = list(rail.geometry.coords)\n",
    "    for coord in [coords[0], coords[-1]]:\n",
    "        if coord not in endpoint_to_rail_idx:\n",
    "            endpoint_to_rail_idx[coord] = []\n",
    "        endpoint_to_rail_idx[coord].append(idx)\n",
    "        all_rail_endpoints.append(coord)\n",
    "\n",
    "# Remove duplicates for KD-tree\n",
    "unique_endpoints = list(set(all_rail_endpoints))\n",
    "endpoint_array = np.array(unique_endpoints)\n",
    "tree = cKDTree(endpoint_array)\n",
    "\n",
    "print(f\"Isolated stations: {len(isolated_stations)}\")\n",
    "print(f\"Unique rail endpoints: {len(unique_endpoints)}\\n\")\n",
    "\n",
    "# Analyze each isolated station\n",
    "for station_id in isolated_stations[:10]:  # First 10\n",
    "    station_data = station_groups[station_id]\n",
    "    station_coords = station_data['coords']\n",
    "    \n",
    "    print(f\"Station '{station_id}' ({station_data['name']}):\")\n",
    "    print(f\"  Platform coords: {len(station_coords)}\")\n",
    "    \n",
    "    # Find nearest rail endpoint to any station coordinate\n",
    "    min_dist = float('inf')\n",
    "    nearest_endpoint = None\n",
    "    nearest_station_coord = None\n",
    "    \n",
    "    for coord in station_coords:\n",
    "        dist, idx = tree.query(coord)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            nearest_endpoint = unique_endpoints[idx]\n",
    "            nearest_station_coord = coord\n",
    "    \n",
    "    # Convert to approximate meters (at Japan's latitude ~35°)\n",
    "    dist_meters = min_dist * 111000 * 0.9  # rough conversion\n",
    "    \n",
    "    print(f\"  Nearest rail endpoint: {min_dist:.8f}° (~{dist_meters:.1f}m)\")\n",
    "    print(f\"    Station coord: {nearest_station_coord}\")\n",
    "    print(f\"    Rail endpoint: {nearest_endpoint}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Export the graph\n",
    "output_dir = Path('../datasets/japan')\n",
    "output_path = output_dir / 'japan_rail_network.gpickle'\n",
    "\n",
    "print(f\"Exporting graph to {output_path}...\")\n",
    "\n",
    "with output_path.open('wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "\n",
    "print(f\"✓ Graph exported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Visualize the graph (Swiss-style with station/infrastructure layers)\n",
    "print(\"Creating interactive map...\")\n",
    "\n",
    "# Calculate center of all nodes\n",
    "all_lats = [data['lat'] for _, data in G.nodes(data=True) if data.get('lat')]\n",
    "all_lons = [data['lon'] for _, data in G.nodes(data=True) if data.get('lon')]\n",
    "center_lat = sum(all_lats) / len(all_lats)\n",
    "center_lon = sum(all_lons) / len(all_lons)\n",
    "\n",
    "# Create base map\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=6, tiles='CartoDB Positron')\n",
    "\n",
    "# Define colors (similar to Swiss map)\n",
    "STATION_COLOR = '#1f77b4'  # Blue for stations\n",
    "INFRA_COLOR = '#ff7f0e'    # Orange for infrastructure\n",
    "EDGE_COLOR = '#6c757d'     # Gray for edges\n",
    "ISOLATED_COLOR = '#d62728' # Red for isolated nodes\n",
    "\n",
    "# Create feature groups for layers\n",
    "edges_fg = folium.FeatureGroup(name='Edges', show=True)\n",
    "stations_fg = folium.FeatureGroup(name='Stations', show=True)\n",
    "infra_fg = folium.FeatureGroup(name='Infrastructure', show=False)\n",
    "isolated_fg = folium.FeatureGroup(name='Isolated Stations', show=True)\n",
    "\n",
    "# Identify isolated stations\n",
    "isolated_nodes = set()\n",
    "for comp in components:\n",
    "    if len(comp) == 1:\n",
    "        node = list(comp)[0]\n",
    "        if G.nodes[node].get('node_type') == 'station':\n",
    "            isolated_nodes.add(node)\n",
    "\n",
    "# Add edges\n",
    "for u, v, data in G.edges(data=True):\n",
    "    u_data = G.nodes[u]\n",
    "    v_data = G.nodes[v]\n",
    "    \n",
    "    if u_data.get('lat') and v_data.get('lat'):\n",
    "        folium.PolyLine(\n",
    "            [[u_data['lat'], u_data['lon']], [v_data['lat'], v_data['lon']]],\n",
    "            color=EDGE_COLOR,\n",
    "            weight=1,\n",
    "            opacity=0.6,\n",
    "        ).add_to(edges_fg)\n",
    "\n",
    "# Add nodes\n",
    "for node, data in G.nodes(data=True):\n",
    "    if not data.get('lat'):\n",
    "        continue\n",
    "    \n",
    "    node_type = data.get('node_type', 'unknown')\n",
    "    is_isolated = node in isolated_nodes\n",
    "    \n",
    "    if is_isolated:\n",
    "        color = ISOLATED_COLOR\n",
    "        layer = isolated_fg\n",
    "        radius = 6\n",
    "    elif node_type == 'station':\n",
    "        color = STATION_COLOR\n",
    "        layer = stations_fg\n",
    "        radius = 4\n",
    "    else:\n",
    "        color = INFRA_COLOR\n",
    "        layer = infra_fg\n",
    "        radius = 3\n",
    "    \n",
    "    # Create popup with node info\n",
    "    popup_html = f\"\"\"\n",
    "    <b>{data.get('name', node)}</b><br>\n",
    "    ID: {node}<br>\n",
    "    Type: {node_type}<br>\n",
    "    \"\"\"\n",
    "    if node_type == 'station':\n",
    "        popup_html += f\"Operators: {', '.join(data.get('operators', ['N/A']))}<br>\"\n",
    "        popup_html += f\"Platforms: {data.get('platform_count', 'N/A')}<br>\"\n",
    "        if data.get('all_names') and len(data['all_names']) > 1:\n",
    "            popup_html += f\"Alt names: {', '.join(data['all_names'])}<br>\"\n",
    "    popup_html += f\"Degree: {G.degree(node)}\"\n",
    "    if is_isolated:\n",
    "        popup_html += \"<br><b style='color:red'>⚠️ ISOLATED</b>\"\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[data['lat'], data['lon']],\n",
    "        radius=radius,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.9 if is_isolated else 0.7,\n",
    "        weight=1,\n",
    "        tooltip=f\"{data.get('name', node)} ({node})\",\n",
    "        popup=folium.Popup(popup_html, max_width=300),\n",
    "    ).add_to(layer)\n",
    "\n",
    "# Add layers to map\n",
    "edges_fg.add_to(m)\n",
    "stations_fg.add_to(m)\n",
    "infra_fg.add_to(m)\n",
    "isolated_fg.add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "# Summary\n",
    "station_count = sum(1 for _, d in G.nodes(data=True) if d.get('node_type') == 'station')\n",
    "infra_count = sum(1 for _, d in G.nodes(data=True) if d.get('node_type') == 'infrastructure')\n",
    "\n",
    "print(f\"✓ Map created:\")\n",
    "print(f\"  Stations: {station_count:,} (isolated: {len(isolated_nodes)})\")\n",
    "print(f\"  Infrastructure: {infra_count:,}\")\n",
    "print(f\"  Edges: {G.number_of_edges():,}\")\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_final_project)",
   "language": "python",
   "name": "venv_final_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
