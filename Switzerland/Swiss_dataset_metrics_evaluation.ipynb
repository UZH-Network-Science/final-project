{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Swiss Dataset Metrics Evaluation\n",
    "\n",
    "This notebook loads the complete Swiss rail network (Swisstopo data) and calculates key metrics defined in the proposal (Efficiency and Resilience).\n",
    "\n",
    "Metrics calculated:\n",
    "1. **Global Efficiency**\n",
    "2. **Local Efficiency**\n",
    "3. **Average Clustering Coefficient**\n",
    "4. **Average Path Length** (on Largest Connected Component)\n",
    "5. **Resilience** (Robustness to random node failures)\n",
    "6. **Efficiency Decay - Random Failure**\n",
    "7. **Efficiency Decay - Targeted Attacks** (Degree and Betweenness Centrality)\n",
    "\n",
    "Results are stored in `swiss_metrics_results.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"swisstopo\"\n",
    "BASE_DIR = Path(\"../datasets/switzerland\")\n",
    "SWISSTOPO_GDB_PATH = BASE_DIR / \"schienennetz_2056_de.gdb\"\n",
    "METRICS_OUTPUT_PATH = \"../metrics/switzerland/swiss_metrics_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_lines(geom):\n",
    "    if geom is None or geom.is_empty:\n",
    "        return []\n",
    "    coords = []\n",
    "    if geom.geom_type == 'LineString':\n",
    "        coords.extend((pt[1], pt[0]) for pt in geom.coords)\n",
    "    elif geom.geom_type == 'MultiLineString':\n",
    "        for line in geom.geoms:\n",
    "            coords.extend((pt[1], pt[0]) for pt in line.coords)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Swisstopo data...\n",
      "Building Graph...\n",
      "Graph built: 3210 nodes, 3377 edges\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Swisstopo data...\")\n",
    "net_segments = gpd.read_file(SWISSTOPO_GDB_PATH, layer='Netzsegment')\n",
    "net_nodes = gpd.read_file(SWISSTOPO_GDB_PATH, layer='Netzknoten')\n",
    "\n",
    "nodes_gdf = net_nodes.to_crs(4326)\n",
    "segments_gdf = net_segments # Assuming already compatible or will be used for topology\n",
    "# segments_wgs84 = segments_gdf.to_crs(4326) # Not strictly needed for topology construction unless extracting coords\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "print(\"Building Graph...\")\n",
    "for _, row in nodes_gdf.iterrows():\n",
    "    node_id = row['xtf_id']\n",
    "    label = row.get('Betriebspunkt_Name') or node_id\n",
    "    lat = row.geometry.y\n",
    "    lon = row.geometry.x\n",
    "    G.add_node(node_id, label=label, lat=lat, lon=lon, source='swisstopo')\n",
    "\n",
    "for _, row in segments_gdf.iterrows():\n",
    "    u = row['rAnfangsknoten']\n",
    "    v = row['rEndknoten']\n",
    "    if pd.isna(u) or pd.isna(v):\n",
    "        continue\n",
    "    if u not in G.nodes or v not in G.nodes:\n",
    "        # print(f\"Skipping edge {u}-{v} (nodes not found)\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate length if possible, otherwise use 1 or shape length if available\n",
    "    weight = row.geometry.length if hasattr(row.geometry, 'length') else 1.0\n",
    "    \n",
    "    G.add_edge(u, v, weight=weight, source='swisstopo')\n",
    "\n",
    "print(f\"Graph built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest Connected Component: 1687 nodes, 1843 edges\n"
     ]
    }
   ],
   "source": [
    "# Metrics usually make sense on the largest connected component for transport networks\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "G_lcc = G.subgraph(largest_cc).copy()\n",
    "print(f\"Largest Connected Component: {G_lcc.number_of_nodes()} nodes, {G_lcc.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "calc_efficiency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Efficiency Metrics (this may take a while)...\n",
      "Global Efficiency: 0.0282\n",
      "Local Efficiency: 0.0233\n",
      "Avg Path Length (Topological): 58.3268\n",
      "Avg Path Length (Weighted): 147601.6278\n",
      "Avg Clustering Coefficient: 0.0223\n",
      "Done in 2.85 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Efficiency Metrics (this may take a while)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Global Efficiency\n",
    "global_eff = nx.global_efficiency(G_lcc)\n",
    "print(f\"Global Efficiency: {global_eff:.4f}\")\n",
    "\n",
    "# Local Efficiency\n",
    "local_eff = nx.local_efficiency(G_lcc)\n",
    "print(f\"Local Efficiency: {local_eff:.4f}\")\n",
    "\n",
    "# Average Path Length\n",
    "avg_path_len = nx.average_shortest_path_length(G_lcc, weight='weight') # Weighted by distance usually\n",
    "avg_path_len_topo = nx.average_shortest_path_length(G_lcc) # Topological\n",
    "print(f\"Avg Path Length (Topological): {avg_path_len_topo:.4f}\")\n",
    "print(f\"Avg Path Length (Weighted): {avg_path_len:.4f}\")\n",
    "\n",
    "# Clustering Coefficient\n",
    "avg_clustering = nx.average_clustering(G_lcc)\n",
    "print(f\"Avg Clustering Coefficient: {avg_clustering:.4f}\")\n",
    "\n",
    "print(f\"Done in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "calc_robustness_random",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Resilience (Random Failures - LCC Size)...\n",
      "Fraction removed: 0.0, LCC Size (relative): 1.0000\n",
      "Fraction removed: 0.05, LCC Size (relative): 0.5780\n",
      "Fraction removed: 0.1, LCC Size (relative): 0.2831\n",
      "Fraction removed: 0.2, LCC Size (relative): 0.0529\n",
      "Fraction removed: 0.3, LCC Size (relative): 0.0247\n",
      "Fraction removed: 0.4, LCC Size (relative): 0.0145\n",
      "Fraction removed: 0.5, LCC Size (relative): 0.0101\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Resilience (Random Failures - LCC Size)...\")\n",
    "def calculate_robustness(graph, fraction, num_simulations=10):\n",
    "    results = []\n",
    "    n = graph.number_of_nodes()\n",
    "    num_to_remove = int(n * fraction)\n",
    "    \n",
    "    for _ in range(num_simulations):\n",
    "        G_temp = graph.copy()\n",
    "        nodes_to_remove = np.random.choice(list(G_temp.nodes()), num_to_remove, replace=False)\n",
    "        G_temp.remove_nodes_from(nodes_to_remove)\n",
    "        if G_temp.number_of_nodes() > 0:\n",
    "            largest_cc = max(nx.connected_components(G_temp), key=len)\n",
    "            results.append(len(largest_cc) / n) # Relative size of LCC\n",
    "        else:\n",
    "            results.append(0.0)\n",
    "    return np.mean(results)\n",
    "\n",
    "fractions = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "robustness_scores = {}\n",
    "\n",
    "for f in fractions:\n",
    "    score = calculate_robustness(G_lcc, f)\n",
    "    robustness_scores[f] = score\n",
    "    print(f\"Fraction removed: {f}, LCC Size (relative): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc_efficiency_decay_random",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Efficiency Decay (Random Failure)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Efficiency Decay (Random Failure)...\")\n",
    "def calculate_efficiency_decay_random(graph, fraction, num_simulations=1000):\n",
    "    results = []\n",
    "    n = graph.number_of_nodes()\n",
    "    num_to_remove = int(n * fraction)\n",
    "    \n",
    "    for _ in range(num_simulations):\n",
    "        G_temp = graph.copy()\n",
    "        nodes_to_remove = np.random.choice(list(G_temp.nodes()), num_to_remove, replace=False)\n",
    "        G_temp.remove_nodes_from(nodes_to_remove)\n",
    "        \n",
    "        eff = nx.global_efficiency(G_temp)\n",
    "        results.append(eff)\n",
    "        \n",
    "    return np.mean(results)\n",
    "\n",
    "eff_decay_random = {}\n",
    "for f in fractions:\n",
    "    score = calculate_efficiency_decay_random(G_lcc, f)\n",
    "    eff_decay_random[f] = score\n",
    "    print(f\"Fraction removed: {f}, Global Eff (Random): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc_efficiency_decay_targeted",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Efficiency Decay (Targeted Attacks)...\n",
      "Calculating centrality measures...\n",
      "Fraction removed: 0.0 | Degree Target Eff: 0.0282 | Betweenness Target Eff: 0.0282\n",
      "Fraction removed: 0.05 | Degree Target Eff: 0.0066 | Betweenness Target Eff: 0.0191\n",
      "Fraction removed: 0.1 | Degree Target Eff: 0.0039 | Betweenness Target Eff: 0.0148\n",
      "Fraction removed: 0.2 | Degree Target Eff: 0.0021 | Betweenness Target Eff: 0.0088\n",
      "Fraction removed: 0.3 | Degree Target Eff: 0.0020 | Betweenness Target Eff: 0.0063\n",
      "Fraction removed: 0.4 | Degree Target Eff: 0.0019 | Betweenness Target Eff: 0.0052\n",
      "Fraction removed: 0.5 | Degree Target Eff: 0.0020 | Betweenness Target Eff: 0.0049\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Efficiency Decay (Targeted Attacks)...\")\n",
    "\n",
    "# Pre-calculate centralities for Static strategies\n",
    "print(\"Calculating centrality measures...\")\n",
    "degree_cent = nx.degree_centrality(G_lcc)\n",
    "betweenness_cent = nx.betweenness_centrality(G_lcc) # Expensive, but done once\n",
    "\n",
    "sorted_nodes_degree = sorted(degree_cent, key=degree_cent.get, reverse=True)\n",
    "sorted_nodes_betweenness = sorted(betweenness_cent, key=betweenness_cent.get, reverse=True)\n",
    "\n",
    "def calculate_efficiency_decay_targeted(graph, fraction, sorted_nodes_list):\n",
    "    n = graph.number_of_nodes()\n",
    "    num_to_remove = int(n * fraction)\n",
    "    \n",
    "    nodes_to_remove = sorted_nodes_list[:num_to_remove]\n",
    "    \n",
    "    G_temp = graph.copy()\n",
    "    G_temp.remove_nodes_from(nodes_to_remove)\n",
    "    \n",
    "    return nx.global_efficiency(G_temp)\n",
    "\n",
    "eff_decay_degree = {}\n",
    "eff_decay_betweenness = {}\n",
    "\n",
    "for f in fractions:\n",
    "    # Degree Targeted\n",
    "    score_deg = calculate_efficiency_decay_targeted(G_lcc, f, sorted_nodes_degree)\n",
    "    eff_decay_degree[f] = score_deg\n",
    "    \n",
    "    # Betweenness Targeted\n",
    "    score_bet = calculate_efficiency_decay_targeted(G_lcc, f, sorted_nodes_betweenness)\n",
    "    eff_decay_betweenness[f] = score_bet\n",
    "    \n",
    "    print(f\"Fraction removed: {f} | Degree Target Eff: {score_deg:.4f} | Betweenness Target Eff: {score_bet:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to swiss_metrics_results.json\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"num_nodes\": G_lcc.number_of_nodes(),\n",
    "    \"num_edges\": G_lcc.number_of_edges(),\n",
    "    \"global_efficiency\": global_eff,\n",
    "    \"local_efficiency\": local_eff,\n",
    "    \"average_path_length_topological\": avg_path_len_topo,\n",
    "    \"average_path_length_weighted\": avg_path_len,\n",
    "    \"average_clustering_coefficient\": avg_clustering,\n",
    "    \"robustness_random_failure\": robustness_scores,\n",
    "    \"efficiency_decay_random_failure\": eff_decay_random,\n",
    "    \"efficiency_decay_targeted_degree\": eff_decay_degree,\n",
    "    \"efficiency_decay_targeted_betweenness\": eff_decay_betweenness\n",
    "}\n",
    "\n",
    "with open(METRICS_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved to {METRICS_OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}